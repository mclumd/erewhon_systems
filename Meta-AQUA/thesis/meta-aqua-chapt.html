<P><A HREF="git-cc-96-06.book.html"><IMG SRC="fm2html-toc.gif">Table of Contents</A>
<A HREF="learn-chapt.html"><IMG SRC="fm2html-previous.gif">Previous Chapter</A>
<!-- This file was created with the fm2html filter.
The filter is copyright Norwegian Telecom Research and
was programmed by Jon Stephenson von Tetzchner. -->

<TITLE>Introspective - CHAPTER VIII</TITLE>

<H2><A NAME="REF19222"> Part  Four</A><BR>
<A NAME="HDR0">  IMPLEMENTATION AND CONCLUSION</A></H2>


<H2><A NAME="REF93871"> CHAPTER VIII  </A><BR>
<A NAME="HDR1">  META-AQUA</A></H2>

<DL>
<DL>
<DD><I>Total grandeur of a total edifice,
<BR>
<DD>Chosen by an inquisitor of structures
<BR>
<DD>For himself.  He stops upon this threshold
<BR>
<DD>As if the design of all his words takes form
<BR>
<DD>And frame from thinking and is realized.
<P>
<DD></I>_Wallace Stevens (1952), pp. 510-511.
<P></DL>

</DL>


<P>The Meta-AQUA system implements the theory of introspective multistrategy learning presented in the previous chapters by providing a computational realization of the concepts within the theory.  The project of building this implementation has been especially challenging.  Not only does the Meta-AQUA system, like many other programs, have a performance system that manipulates an explicit representation of the world, but in addition it has a learning system that analyzes and learns from the results of the performance.  These results, along with a trace of the performance itself, must also be represented explicitly.  Moreover, the system uses three different representational formalisms: frames, CDs, and predicate logic.  The implementation is hence naturally complex. 
<P>As a result of the system's complexity, deliberate decisions were made to implement only the most significant portions of the theory within the Meta-AQUA program.  For example, the memory system in Meta-AQUA is not a full model of human memory.  Instead, it is a rough approximation to the indexed dynamic memory as described in Schank (1982).  Although the implementation of the memory is crude compared to the elaborate functionality of similar memories in other programs (e.g., CYRUS - Kolodner, 1984; DMAP - Martin, 1990; ANON - Owens, 1990a; SMART - Veloso &amp; Carbonell, 1990), it still is a better cognitive model of memory than those using exhaustive search (e.g., FUNES - Markovitch &amp; Scott, 1988).  

<P>Also, as a matter of pragmatics, the Meta-AQUA implementation does not assume that all possible causes for failures enumerated in <A HREF="repr-chapt.html#REF47582">Chapter IV</A> can occur.  Instead the program restricts the number of causes it considers during blame assignment.<A HREF="meta-aqua-chapt.html#FN1">(1)</A> Although future research intends to expand the scope of blame assignment, the current implementation still far exceeds the number of causes that related computational systems entertain.  For example, the MINERVA learning system (Park &amp; Wilkins, 1990) is a theory revision system similar to Meta-AQUA in its use of explanation and introspection.  However, because MINERVA assumes a consistent knowledge base, brute force search, no input, perfect processes, and well-behaved goals, missing domain knowledge is the only possible cause of failure.  By making such assumptions, the blame-assignment task is circumvented altogether.  All failures are attributed to missing pieces of domain knowledge in its background knowledge.  
<P>A few minor differences exist between IML theory and the embodiment of the theory in the Meta-AQUA system.  These discrepancies will be made explicit in this chapter.  Notwithstanding these differences, the implementation represents a substantial undertaking and will be examined in some detail.  The initial section (<A HREF="meta-aqua-chapt.html#REF63065">8.1</A>) outlines Meta-AQUA's system architecture and its file system.  Subsequent sections examine the performance subsystem (<A HREF="meta-aqua-chapt.html#REF76396">8.2</A>), the input problem generator (<A HREF="meta-aqua-chapt.html#REF63204">8.3</A>), the memory system (<A HREF="meta-aqua-chapt.html#REF54249">8.4</A>), and the learning subsystem (<A HREF="meta-aqua-chapt.html#REF43052">8.5</A>), each in turn.  Examples of the running behavior of the system when processing automatically generated input stories (rather than hand-coded stories) illustrate many sections throughout.  The final section (<A HREF="meta-aqua-chapt.html#REF57764">8.6</A>) closes the chapter with a summary and a brief discussion.  
<H3><A NAME="REF63065"> 8.1   Meta-AQUA System Architecture</A></H3>
<P>Meta-AQUA is a learning system that chooses and combines multiple learning methods from a toolbox of algorithms in order to repair faulty components responsible for failures encountered during the system's performance task.  The program incorporates an introspective version of the AQUA (Ram, 1991, 1993, 1994) story-understanding system as the performance task from which learning can take place.  As a front end module to the performance system, a specially modified version of the Tale-Spin (Meehan, 1981) story-generation program automatically produces input data.  At the back end, the UM Nonlin planning system (Ghosh et al., 1992) creates a learning plan designed to improve the performance.  An extensive frame system (Minsky, 1975; Wilensky, 1986b) was built to provide the formalism with which to represent the system's knowledge, both of the domain and of itself.  This knowledge is stored in and retrieved from a simple indexed memory.  The memory is partitioned into a working memory (FK) and a long-term store (BK).  

<P>The system architecture and flow of information within Meta-AQUA is shown in <A HREF="meta-aqua-chapt.html#REF81671">Figure 70</A>.  The problem generation module outputs a story to the performance system with the initial goal to understand the input (i.e., build a coherent conceptual interpretation).  The story understanding system uses schemas from the BK to build a representation of the story in the FK.  If this task fails, then a trace of the reasoning that preceded the failure is passed to the learning subsystem.  A CBR subsystem within the learner uses past cases of introspective reasoning from the BK to explain the failure and to generate a set of learning goals.  These goals, along with the trace, are then passed to a nonlinear planner.  The planner subsequently builds a learning strategy from its toolbox of learning methods.  The learning plan is then passed to an execution system that examines and changes items in the BK.  These changes enable improved performance in subsequent processing.  
<P><A HREF="meta-aqua-chapt.fig_1.ps" NAME="REF81671"><IMG SRC="meta-aqua-chapt.fig_1.gif"><P> Figure 70.  <B></B>Detailed Meta-AQUA system architecture</A>



<P>Meta-AQUA is programmed in Symbolics Common LISP under the Genera operating system (Version 8.3).  The hardware platform is a Symbolics MacIvory Model-3 LISP microprocessor embedded in a Macintosh IIci personal computer.  Including comments and documentation, the LISP source code takes up approximately 750 kilobytes of disk space in sixty-seven files.  The file system definition is pictured in <A HREF="meta-aqua-chapt.html#REF96417">Figure 71</A>.<A HREF="meta-aqua-chapt.html#FN2">(2)</A> Two of the modules (frame and non-lin) are stand-alone subsystems each of which have their own defsystem constructs.  The four other modules are native to the Meta-AQUA system.  The five main modules (not including documentation) each have a separate symbol package.  The following subsections describe each of the major subsystems in turn.  
<P><A HREF="meta-aqua-chapt.fig_70.ps" NAME="REF96417"><IMG SRC="meta-aqua-chapt.fig_70.gif"><P> Figure 71.  Meta-AQUA file system definition</A>


<H3><A NAME="REF76396"> 8.2   The Performance Subsystem</A></H3>
<P>The AQUA program is a question-driven story understanding system whose task is to explain terrorist activities and events contained in newspaper-like stories provided as input.  The Meta-AQUA system learns about drug-smuggling activities, given AQUA's prior experience with stories about terrorists.  Both systems' performance task is to "understand" stories by building causal explanations that link the individual events into a coherent whole.<A HREF="meta-aqua-chapt.html#FN3">(3)</A> Meta-AQUA adds introspective reasoning and multistrategy learning using Meta-XP structures and the learning theory presented in parts Two and Three of this thesis.  In addition and unlike AQUA, the performance sub-system of Meta-AQUA uses a multistrategy approach to understanding.  Thus, the top-level goal is to choose a comprehension method by which it can understand the input. 


<P>To process input information, the system posts all performance goals and learning goals in a priority queue.  These goals are then processed by the current priority value such that the highest value is pursued first.  For instance, when a new conceptual input arrives, a goal to understand the input is placed in the priority queue with a nominal value.  If no other goals have higher values, then the comprehension goal will be taken from the queue and processed.  The goal to understand an input is then made into a subgoal to detect an anomaly in the input.  
<P>For the task of story understanding, Meta-AQUA employs an algorithm whose flow of control is outlined in <A HREF="meta-aqua-chapt.html#REF27446">Figure 72</A>.  First, the outer loop inputs a sentence representation and checks to see if the concept can answer a prior question.  If it can, the reasoning associated with the question is resumed.  Otherwise, the concept is passed to the understanding algorithm.  The understanding algorithm consists of three phases: anomaly identification, hypothesis generation, and hypothesis verification.  
<P><A HREF="meta-aqua-chapt.fig_22.ps" NAME="REF27446"><IMG SRC="meta-aqua-chapt.fig_22.gif"><P> Figure 72.  Meta-AQUA flow of control</A>


<P>The first phase looks for questions associated with the concept by checking the concept for interesting characteristics.  Meta-AQUA considers acts of sex, violence, and loud noises inherently interesting (Schank &amp; Abelson, 1977).  Moreover, any concept that is anomalous is considered interesting (Ram, 1990b), as is any concept about which the program has recently learned something.  Inherently interesting acts are detected by the concept type of the input.  Anomaly checking is performed by comparing the input to the conceptual definitions found in the conceptual hierarchy.  If a concept contradicts a constraint, an anomaly exists and a question is posed.  Such a question represents a fundamental learning goal (or more specifically, a knowledge acquisition goal).  The goal is to construct or otherwise acquire an explanation of why the anomaly exists.  If no anomaly is detected, the concept is skimmed.  Control then passes back to the beginning.
<P>When an input is skimmed, it is passed to a simplified version of SAM, a script application program (Cullingford, 1978, 1981).  The script applier understands a story by matching input sentences to stereotypical sequences of events (i.e., to scripts).  For example, the simple drug-bust script consists of an initial drug detection, then confiscation of the contraband, followed by the arrest of the person possessing the drugs.  Although scripts omit many of the causal relations between events in a story, they can help an understander interpret a story by providing details not explicitly mentioned in the story.  During the skimming process, however, if an input is not matched with a script, then the input is placed on a list of current story structures in the FK to await further processing.  
<P>As an example of the role scripts play in the input to the understanding process, consider the following.  The pipe-smoking-script contains an instrumental scene (gain-control-of-object) that establishes the preconditions (possession of a pipe) necessary for the goal scene (smoke-pipe) of the script.  The instrumental scene itself has sub-scenes that must be matched to the input, some of which in turn may have additional sub-scenes.  They include the sub-scene open-container (if the pipe is in the cupboard), an ATRANS (to transfer possession to the smoker), and close-container.  But in the event of a match, not only are the sub-events which came from the story examined by the understander to detect anomalies, but the inferred gain-control-of-object structure is input for anomaly detection as well. 


<P>Rather than skimming the input, the system may examine it in more detail by posing questions about particular parts of the input.  If a question is posed, the understander attempts to answer the question by generating a hypothesis.  The basis of the decision to pose a question (i.e., what knowledge is relevant in making the determination) is then recorded in a TMXP.  Strategies for hypothesis generation include application of known explanation patterns ("XP application"), case-based reasoning, and analogy.  If none of these methods applies, then the process is suspended until a later opportunity presents itself.<A HREF="meta-aqua-chapt.html#FN4">(4)</A> 
<P>After a hypothesis is generated, the potential answer passes to the verification phase of the performance task.  Strategies for hypothesis verification include devising a test (currently not implemented), comparison to known concepts, and suspension of the reasoning task.  Following this, the system reviews the chain of reasoning during the verification phase to detect failure.  The failure detection process examines the reasoning trace using the algorithm described by <A HREF="proc-chapt.html#REF39749">Figure 47 on page 121</A>.  If a failure occurs, then control passes to the learning subsystem and further input processing is suspended until control returns.  
<H3><A NAME="REF63204"> 8.3   Input Subsystem</A></H3>
<P>Input to the performance system can originate in two different ways.  Hand-coded stories provide explicit demonstrations of specific features of the IML learning algorithm.  A number of these stories (e.g., HC1 and HC2) have been discussed in previous chapters.  In addition, a modified version of the Tale-Spin (Meehan, 1981) story-generator was integrated into the Meta-AQUA architecture to produce stories automatically.  The function of a separate generator is to attenuate the bias of the programmer who creates hand-coded stories and to provide a means for randomly varying the conditions under which learning takes place.  These two conditions provided by the generator have enabled the design of an empirical study that will be reported in the next chapter.  
<H4><A NAME="HDR2"> 8.3.1  The Tale-Spin Story-Generator </A></H4>

<P>Given a main character and a problem for the character, the Tale-Spin module creates stories by simulating the actions that would be necessary for the character to achieve goals stemming from the problem.  For example, if a character has the problem of being thirsty, Tale-Spin assigns the character an initial goal to remove a state of thirst.  The character can achieve the goal by travelling to where water or drink exists if the location is known.  If it is not known, the character can ask another agent in the story.  The character then gains possession of the drink and finally ingests it.  At selected points, the generator inserts random events into the story.  For each event in the story, the generator adds any associated causal results from the event.  These results change the world and enable further actions by the characters in the story.  For example, the act of travelling to the location of water enables the taking possession of it which in turn enables the drinking of it.  This final action removes the hunger.  The story terminates when the goals and subgoals of the main character have been achieved or when all possible plans to achieve them have been exhausted.  
<P>The Tale-Spin program was obtained from the University of California at Irvine<A HREF="meta-aqua-chapt.html#FN5">(5)</A> where it is used as a problem generator for the OCCAM learning system (Pazzani, 1994).  OCCAM learns about physical causation given stories in which characters perform actions such as playing ball when bored.  So for example, when children accidentally drop a solid ball, it will not break; whereas, when children drop a balloon upon a sharp object (e.g., a rose bush), the object will break.  In this case, OCCAM learns the causal interaction between object composition and surface impact.  In OCCAM's world, four main characters exist.  They are Dad, Mom, and their two children Lynn and Lynn.  The problems that they encounter are hunger, thirst and boredom.  In addition, the house contains a cat who randomly knocks vases from tables to the floor.  Given these initial program conditions, Tale-Spin was extended to produce stories in the domain of criminal activities.  
<H4><A NAME="REF90144"> 8.3.2  The Elvis World</A></H4>
<P>In order to support large data collection, additions to Tale-Spin provide numerous scenarios with a potentially infinite number of variations that test Meta-AQUA's ability to learn from explanation failure.  Among the changes, a musician named Elvis and a police officer were added to the cast of characters.  Elvis is temporarily boarding with Mom, Dad and family, whereas the officer occasionally visits the house, presumably because of neighborhood complaints of loud music and raucous behavior.  Furthermore, the police officer often (but not always) brings a drug-detection dog along with him, and the domestic household now contains a pet dog.  

<P>Two new problem types were also added.  Characters may be <I>jonesing</I><A HREF="meta-aqua-chapt.html#FN6">(6)</A> for drugs.  In Elvis' case, he sometimes smokes marijuana to relieve his jones, whereas Dad occasionally smokes a pipe with tobacco (see <A HREF="symptoms-chapt.html#REF26241">Figure 15 on page 43</A>).  Lynn has also been given a tobacco habit.  The police officer has the problem of being <I>concerned</I> about the law.  The state of being concerned is relieved if he can either locate contraband or arrest criminals.<A HREF="meta-aqua-chapt.html#FN7">(7)</A> The program was also modified to hide the marijuana during story initialization in different locations around the house (e.g, in the cupboard, refrigerator, and under the carpet), so the officer's task varies depending on entry conditions (i.e., at what point in the story the officer arrives on the scene and whether the dog accompanies him), the initial location of the pot, and the actions of the characters in the story.  
<P>Moreover, to facilitate the performance task, the Tale-Spin program was modified so as to generate explanations of key events in the stories.  The resolution of all anomalies are thus incorporated within every story.  For example, Tale-Spin always includes a reason why police dogs bark when generating a story.  Although in an ideal implementation, the understanding process should be able to make powerful enough inferences to confirm explanations of the input independently, the performance task has been simplified within Meta-AQUA.  Instead of using inference to confirm hypotheses, Meta-AQUA mainly depends on the story to provide explanations that confirm them.  For the implementation, the research goal is to concentrate on the learning task rather than the understanding task.  
<P>An example of Tale-Spin output is shown in the story TS1 of <A HREF="meta-aqua-chapt.html#REF42003">Figure 73</A>.  The example is roughly equivalent to the hand-coded story that Chapters VI and VII examined in detail (i.e., the airport drug-bust story, HC1, shown in <A HREF="cbi-chapt.html#REF45989">Figure 52 on page 140</A>).  <A HREF="meta-aqua-chapt.html#REF60932">Figure 74</A>   shows the specific sentences of story TS1 that correspond to the sentences of story HC1.  Unlike the hand-tailored stories, however, the length of Tale-Spin's stories range from 3 to 108 sentences and average approximately 30.  
<P><A HREF="meta-aqua-chapt.fig_61.ps" NAME="REF42003"><IMG SRC="meta-aqua-chapt.fig_61.gif"><P> Figure 73.  Tale-Spin story TS1</A>


<P><A HREF="meta-aqua-chapt.fig_73.ps" NAME="REF60932"><IMG SRC="meta-aqua-chapt.fig_73.gif"><P> Figure 74.  Sentences from story TS1 corresponding to HC1</A>


<P>Unlike AQUA, the Meta-AQUA story understanding subsystem does not actually parse the sentences from an English representation.  Because the focus of this research does not center on the natural language understanding problem, Meta-AQUA assumes that input sentences are already represented conceptually (i.e., Tale-Spin pre-parses them).  The mumble module of Tale-Spin provides stylized English paraphrases to assist the system user.


<H4><A NAME="REF50302"> 8.3.3  Interface to the Performance System </A></H4>
<P>As seen in <A HREF="meta-aqua-chapt.html#REF48323">Figure 75</A>, the interface between Tale-Spin and the performance system is simple.  The spin function of Tale-Spin takes a character and problem to generate a CD representation of the story.  The mumble function generates the English equivalent of the CD conceptual representation.  Both of these outputs are then placed on the global variable <CODE>*ALL*</CODE>.  This variable is a list of tuples of the form &lt;CD "generated equivalent text"&gt;.  A translation routine (function convert-story) then converts the CDs into a frame representation used by Meta-AQUA.  The result is placed on the global list <CODE>*Story-Concepts*</CODE>.  This structure is a list of tuples of the form &lt;frame "equivalent text"&gt;.  Then for each input concept, the function init-goals creates a knowledge acquisition goal to understand the concept.  The function places each goal on the <CODE>*Goal-Queue*</CODE> priority queue.  In turn, the performance system evaluates each input, places the interpreted result on the variable <CODE>*World-Model*</CODE> (so it can easily be displayed at the end of the program as output), and indexes the result in the FK.
<P><A HREF="meta-aqua-chapt.fig_24.ps" NAME="REF48323"><IMG SRC="meta-aqua-chapt.fig_24.gif"><P> Figure 75.  Representational flow between generator and understander</A>


<P>Before we examine the memory system, it should be made explicit that although there is a tight interface between the Tale-Spin story generator and the Meta-AQUA story understanding subsystem, the implementation of and theory behind the Tale-Spin generator in no way reflects the content or claims concerning IML theory.  On the contrary, the manner in which input is created for the performance system is largely incidental to the theory and the implementation of the rest of the Meta-AQUA system.  Meehan (1981) asserts that stories should be both interesting and coherent.  Interesting stories set up a focal problem domain that span a number of levels.  Coherent stories contain rational characters that pursue individual goals that then interact.  Moreover, Meehan claimed that story simulation itself was a form of cognition (p. 203).  None of these theoretical underpinnings directly impinge on the theory of learning presented here.  We do not subscribe to nor refute any such claims.  The program is used simply as a matter of computational convenience and to generate less biased input.  
<H3><A NAME="REF54249"> 8.4   Memory</A></H3>
<P>Computer memory is often viewed as a virtually error-free medium in which retrieval of data is performed by simple fetch operations.  As computer memories grow, however, brute-force search for the address to perform the fetch becomes increasingly intractable.  Memory indexing is added in order to make memory retrieval more efficient.  A memory-indexing mechanism is a trade-off between time to search and accuracy of retrieval; though efficiency is gained, poor indexing schemes risk not finding the proper information.  Indexes are pointers from some feature in the environment (cue) to a memory element associated with that feature.  


<P>The memory for Meta-AQUA is partitioned into two indexed memories: a working memory called the foreground knowledge, or FK, and a long-term store called the background knowledge, or BK.  The BK used in the current implementation consists of a multiple-inheritance conceptual hierarchy, a case library of past episodes, a set of story scripts, and an indexed collection of XPs.  The FK is a dynamic list of structures representing the current understanding of the story or "world model." Although both memories are implemented as lists of LISP symbols that have particular values, all processes other than printing and bookkeeping functions access memory by matching the cues chosen at retrieval time with the indexes created at storage time.  
<P>This section explains the representation used for items stored in memory (<A HREF="meta-aqua-chapt.html#REF18568">Section 8.4.1</A>), describes the implementation used for indexing items in memory and in retrieving them (<A HREF="meta-aqua-chapt.html#REF83198">Section 8.4.2</A>), and works through a short story that Tale-Spin generates in order to demonstrate the advantages of opportunistic memory in the Meta-AQUA system (<A HREF="meta-aqua-chapt.html#REF51173">Section 8.4.3</A>).  The representation, indexing scheme and the structure of the memory subsystem allows the generation of multiple hypotheses and their respective verification to be interleaved in arbitrary order.  The example also shows that Meta-AQUA can benefit from learning even before it is finished processing the story in which the learning takes place.
<H4><A NAME="REF18568"> 8.4.1  Types and Tokens</A></H4>
<P>The distinction between a <I>type</I> and a <I>token</I> is an important difference maintained in Meta-AQUA's memory.  Types represent conceptual categories and are defined through the frame-based knowledge-representation system underlying the Meta-AQUA program.  Tokens are instantiated instances (reified types).  For example, Meta-AQUA's BK contains a type definition associated with the act of barking by dogs.  This type captures in general the assumed constraint that dogs bark at animate objects (see <A HREF="symptoms-chapt.html#REF0">page 56</A> for an abbreviated type definition for dog-barks).  Contrastingly, as Meta-AQUA receives input, it instantiates a token for the each event or assertion, including the episode of a particular dog barking at a particular inanimate object.  These tokens are incorporated into retrieved cases, scripts, or XPs and stored in the FK.  

<P>A token is represented in memory as a <I>frame variable</I>.  A frame variable is a unique LISP symbol, such as the gensym XP-GOAL-OF-OUTCOME-&gt;ACTOR.115.<A HREF="meta-aqua-chapt.html#FN8">(8)</A> The symbol value of a frame variable is either a <I>frame form</I> or a <I>literal frame</I>.  A literal frame is an arbitrary structured symbol-value, whereas a frame form is represented as a list consisting of a <I>frame-type designator </I>followed by zero or more <I>slots</I> (see <A HREF="meta-aqua-chapt.html#REF23036">Figure 76</A>).  Literals are distinguished from other frame variables in that routines which traverse frame structures cannot inspect or otherwise traverse a literal.<A HREF="meta-aqua-chapt.html#FN9">(9)</A> Slots are attribute-value relations (sometimes called slot-filler pairs) having a particular name (i.e., the <I>role</I> of the slot) and an arbitrary number of <I>facets</I>.  A distinguished facet for every slot is the <I>value facet</I>, representing the <I>role-filler</I> (or simply <I>filler</I>) of the slot.  Facet values may be either an <I>attribute value</I>, a frame, or list of frames.  Attribute values are special terminal frames that specify a member of an enumerated set.  For example, green.0 is an attribute value of the enumerated type color-value.  It fills the value facet of a color slot and evaluates to the slotless frame form "(green)".  As mentioned in <A HREF="repr-chapt.html#REF62779">Section 4.4</A>, slots themselves are treated in the frame system as a first-class objects, and thus have explicit frame representations (see <A HREF="repr-chapt.html#REF46420"> Figure 25., "Relations as first-class objects," starting on page 81</A>).  Thus, the relation facet has as its filler a frame representing the attribute-value relation.<A HREF="meta-aqua-chapt.html#FN10">(10)</A>  
<P><A HREF="meta-aqua-chapt.fig_76.ps" NAME="REF23036"><IMG SRC="meta-aqua-chapt.fig_76.gif"><P> Figure 76.  Generalized frame token structure</A>



<P>A special property of the knowledge representation system is that type definitions (categories) are stored as the symbol values of the frame-type designator.  As a result, every token has close proximity to its corresponding type.  Given a frame token as shown in <A HREF="meta-aqua-chapt.html#REF23036">Figure 76</A>, the type definition can be found by taking the symbol-value of the symbol 'FRAME-TYPE.  Thus in general, to find the conceptual definition of an arbitrary frame variable, X, the LISP call (symbol-value (first (symbol-value X))) will suffice.<A HREF="meta-aqua-chapt.html#FN11">(11)</A>
<H4><A NAME="REF83198"> 8.4.2  Indexing</A></H4>
<P>Conceptually, an index is a mapping from a context to a memory item.  Indexes describe a situation in a story or in reasoning about a story and are typed according to the role they play in reasoning. Depending on the index type (question-type.0, xp-type.0, case-type.0, or plan-type.0), indexes map to different conceptual memory items (questions, xps, cases and plans, respectively). In Meta-AQUA, each index is composed of simple chains of <I>micro-indexes </I>(see Bhatta, 1995; Kolodner, 1993, pp. 193-245; and Owens, 1993 for descriptions of more sophisticated index implementations).
<P>For example when processing story TS1, Meta-AQUA poses the question "why did the dog bark at the vase?" It hypothesizes that the vase somehow threatens the police dog, but because it cannot verify this, the system suspends the deliberation and indexes the question into memory.  The index used to store the question is an index of type question-type.0 based on a dog being the actor of the barking event and vase being the object.  The choice of features is determined by the path along which the anomaly is discovered in the input structure.  When it later examines the explanation that the dog barked because it detected drugs, the input causes a reminding that resumes the prior verification process.  The composition of these types of indexes is shown in <A HREF="meta-aqua-chapt.html#REF40382">Figure 77</A>.  
<P><A HREF="meta-aqua-chapt.fig_64.ps" NAME="REF40382"><IMG SRC="meta-aqua-chapt.fig_64.gif"><P> Figure 77.  Indexing for items about why dogs bark</A>


<P>Indexing structure in Meta-AQUA is composed of three levels.  The primary level of all indexing is specified by the triple &lt;relation, predicate, value&gt;, that is, by a relation, the relation's <CODE>domain</CODE>, and the relation's <CODE>co-domain</CODE>.  An example triple would be &lt;actor, bark, dog&gt;.  In fact, the actor relation is an important and salient relation in general because it answers the question "who did what?" The relation distinguishes, for example, explanations for why dogs bark from those explaining why seals bark.  As seen in <A HREF="meta-aqua-chapt.html#REF40382">Figure 77</A>, this index level is represented by placing a micro-index on the bark property of the actor symbol.  A micro-index is simply a "gensym"<A HREF="meta-aqua-chapt.html#FN12">(12)</A> that is guaranteed to be unique.  The micro-index is then given a dog property whose value points to the next level of indexing.


<P>A second level of indexing is specified by zero (or more) slots of the original relation's predicate.  Each slot is represented by the tuple &lt;role value&gt;.  For each slot, the index requires two micro-indexes.  The first (i.e., role) represents the name of the slot.  The second (i.e., value) represents one of three conditions.  If the role-filler is an attribute value or literal, it represents the frame-value of the filler.  If the role-filler is a relation, it represents the frame-type of the domain of the relation.  Otherwise, it represents the frame-type of the role-filler.  As an example, consider the object at which a dog barks.  The filler of the to slot is the relation at-location whose domain is a container token.  Thus, the index tuple is &lt;to container&gt;.  See the level two portion of <A HREF="meta-aqua-chapt.html#REF40382">Figure 77</A>.  
<P>The final level of indexing specifies the index type.  That is, the index specifies the kind of memory at which it points.  At the end of a sequence for an index, the retrieve or store (index) routine looks at or sets the memory-type.  This level is represented with a final micro-index whose property is one of the following: either question-type.0, xp-type.0, plan-type.0, or case-type.0.  The value of the property will be the element (or a list of elements) stored in memory (e.g., an XP for indexes of type xp-type.0).  
<P>The function do-index is the major memory storage function of Meta-AQUA.  It takes a memory item to be indexed, the item's type classification, and a relation that serves as context for the mapping, and returns an instantiated index frame that represents the index.  As a side-effect, it places the memory item in conceptual memory via the micro-indexing implementational scheme described above.  Optionally, the memory item may be placed in conceptual memory along with any other structures that happen to be there, or it may overwrite what is already there, depending on the caller of the function.  That is, destructive storage is optional.  
<P>In addition, every memory item added to conceptual memory through indexing is placed on a "retrieval list" for finding similar items when storing.  Before a new item is indexed, the memory system performs a check on the retrieval list for the memory item's type to see if there already exists an item that is of this type.  This action simulates a reminding at storage time so that the memory system can find forgotten or lost memories.  This feature is necessary for forgotten goal and missing association errors.  To make these features more concrete, the next section examines a specific example of opportunistic remindings.  See also Section<A HREF="meta-aqua-chapt.html#REF83295"> 8.5.4, "Forgetting a Learned Explanation," starting on page 203</A>.
<H4><A NAME="REF51173"> 8.4.3  Remindings in Opportunistic Memory</A></H4>

<P>Consider the short story generated by Tale-Spin, TS2, as shown in <A HREF="meta-aqua-chapt.html#REF72541">Figure 78</A>.<A HREF="meta-aqua-chapt.html#FN13">(13)</A> Given that Meta-AQUA believes that playing is constrained to children and that people hit or strike animate objects when they wish to hurt them, TS2 will generate a number of anomalies.  When sentence S8 states that Dad plays with a ball, Meta-AQUA detects an anomaly because the token conflicts with its conceptual definition of playing.  That is, it expects only children to be actors of playing events.  Likewise, S9 conflicts with its knowledge of what kinds of objects people hit.  People hitting animate objects cause Meta-AQUA to explain the action.  The first anomaly will cause Meta-AQUA to reach an impasse because it does not have any explanation for why people play.  The second anomaly will cause Meta-AQUA to hypothesize that Lynn wanted to hurt the ball.  Neither of these anomalies can be resolved, so the program suspends them both in the BK.  The first is indexed under adults who play with balls, whereas the second is indexed by children who hit animate objects.<A HREF="meta-aqua-chapt.html#FN14">(14)</A> 
<P><A HREF="meta-aqua-chapt.fig_27.ps" NAME="REF72541"><IMG SRC="meta-aqua-chapt.fig_27.gif"><P> Figure 78.  Tale-Spin story TS2</A>


<P>The story now supplies an explanation for why Lynn hit the ball in sentence S10.  When this explanation arrives, it causes a reminding of the previous question "Why did Lynn hit the ball?" It finds this previous question in the BK and re-establishes it in the FK.  The old question is located because the EXPLAINS node of the input XP (i.e., its consequent) is the hitting event that matches the index under which it was previously stored.<A HREF="meta-aqua-chapt.html#FN15">(15)</A> The explanation contradicts the expectation that Lynn wanted to hurt the ball, so as with the learning episode presented in Chapters <A HREF="cbi-chapt.html#REF37707">VI</A> and <A HREF="learn-chapt.html#REF37707">VII</A>, Meta-AQUA is able to learn the new explanation, loosen its constraint on the objects at which people hit, and differentiate the two hitting explanations by re-indexing them with respect to each other.  

<P>As a result of this learning experience, when the program processes sentence S11, the concept is no longer anomalous.  However, it is interesting to the system because it had recently learned about people hitting animate objects.  This causes Meta-AQUA to explain the action.  Instead of retrieving the old hurt explanation, the system applies the new explanation which is now indexed by person-hit-toy-object.  The explanation is verified when S12 is encountered.  Thus, not only does Meta-AQUA not repeat a failure, but it predicts the correct explanation before encountering it.  Finally, when sentence S13 is processed, the system simply acquires the new explanation.  
<P>Story TS2 is significant for at least two reasons.  For all three of the events (one play and two hit actions), the explanations for why the actors performed the action are given in the story, but they come at different points in the story.  In particular, the explanation that answers why Dad plays with toy balls comes after the explanations for both hitting events even though the question "Why did Dad play with the ball?" was formed first.  Opportunistic memory allows individual questions to be processed independently of the order in which the story provides new information.  That is, opportunism allows interleaving of multiple hypothesis formations and verifications.  
<P>Secondly, this story represents a situation where Meta-AQUA actually benefits from learning within the same story in which learning occurs.  There is no requirement that the entire story be processed before learning takes place.  This incremental form of learning is more cognitively plausible than non-incremental learning systems (e.g., AUTOCLASS, Cheeseman, Kelly, Self, Stutz, Taylor, &amp; Freeman, 1988) that process all input items before generalization or performing other forms of learning.  The following section describes features of the learning system in more detail and provides an extended Tale-Spin example.  
<H3><A NAME="REF43052"> 8.5   Learning Subsystem</A></H3>
<P>The three chapters of <A HREF="proc-chapt.html#REF75364">Part Three</A>, "<A HREF="proc-chapt.html#REF66864">A PROCESS THEORY OF LEARNING AND INTROSPECTION</A>," have already described in detail the bulk of Meta-AQUA's learning mechanisms and algorithms.  The two chapters of <A HREF="symptoms-chapt.html#REF99066">Part Two</A>, "<A HREF="symptoms-chapt.html#REF75230">A CONTENT THEORY OF MENTAL REPRESENTATION</A>," examined the major representations used by the learning system.  This section looks somewhat closer at a few features not covered in this previous material.  <A HREF="meta-aqua-chapt.html#REF68869">Section 8.5.1</A> reviews the basic architecture of the learning system and enumerates the available learning algorithms in the system's toolbox of learning methods.  It also lists the IMXPs used by the system during learning.  <A HREF="meta-aqua-chapt.html#REF10980">Section 8.5.2</A> discusses the space of failure causes that the implemented blame-assignment procedure considers when actually explaining a reasoning failure.  <A HREF="meta-aqua-chapt.html#REF28403">Section 8.5.3</A> briefly discusses learning higher-order knowledge in the context of an Elvis World example.  Finally, <A HREF="meta-aqua-chapt.html#REF83295">Section 8.5.4</A> works through the Tale-Spin version of example HC2, originally presented in <A HREF="theories-chapt.html#REF42614">Section 2.1.2</A>.  

<H4><A NAME="REF68869"> 8.5.1  Divisions of the Learner</A></H4>
<P>The learning subsystem performs four functions.  When the performance task fails, the learning system performs blame-assignment, decides what to learn, constructs a learning strategy, and then executes the strategy.  The learner is partitioned into two parts.  A case-based reasoning component performs the first two functions while a nonlinear planner performs the last two.  As discussed in <A HREF="cbi-chapt.html#REF37707">Chapter VI</A>, the CBR module receives input in the form of a TMXP.  The TMXP represents a trace of the failed reasoning detected during a previous performance task.  During explanation of the prior reasoning (blame assignment), it retrieves an IMXP that helps locate causes of the failure and links the symptoms of failure with the faults.  The IMXP also helps spawn a set of specific learning goals or changes to the system's BK (deciding what to learn).  These goals are then passed to a non-linear planner, UM Nonlin v.1.2<A HREF="meta-aqua-chapt.html#FN16">(16)</A> (Ghosh et al, 1992) along with a predicate representation of the reasoning context.  As discussed in <A HREF="learn-chapt.html#REF37707">Chapter VII</A>, the planner creates a learning plan with which to achieve the learning goals (learning-strategy construction).  To do this the learner uses schemas that are similar to the STRIPS planning-schemas designed for the Blocks World.  The plan is executed by performing calls to particular learning algorithms specified by primitive actions in the learning plan (learning-strategy execution).  Following the learning session, control is passed back to the performance system. 
<P>As mentioned, the learning system has access to a toolbox of learning algorithms from which the Nonlin component creates a learning plan.  This toolbox includes a number of algorithms that were re-implemented so that they operate on a frame representation of conceptual entities used by the system.  The algorithms currently contained in the toolbox are case-acquisition, explanation-based generalization (EBG), abstraction, and index learning.  None of these algorithms perform the same task, so once the system identifies that a learning goal is necessary, it is unambiguous which method applies to the goal.  That is, this research does not address the selective-superiority problem (Brodley, 1993).  Instead, the research examines how to order and select learning methods at a coarse grain level in order to create a learning strategy that avoids learning-goal interactions.  Future research will address the selective superiority problem and will incorporate more methods into the system's toolbox (See <A HREF="future-res-chapt.html#REF59591">Section 10.1.3 on page 248</A>).
<P>The learning system has access to nine IMXPs that drive the learning process.  The IMXPs are as follows:


<DL>
<DL>
<DL>
<DD>IMXP-SUCCESSFUL-PREDICTION
<BR>
<DD>IMXP-EXPECTATION-FAILURE
<BR>
<DD>IMXP-RETRIEVAL-FAILURE
<BR>
<DD>IMXP-NOVEL-SITUATION
<BR>
<DD>IMXP-NOVEL-SITUATION-ALTERNATIVE-REFUTED
<BR>
<DD>IMXP-NOVEL-SITUATION-ALTERNATIVE-REFUTED-NO-ANOMALY
<BR>
<DD>IMXP-BAFFLED-AND-RESOLVED
<BR>
<DD>IMXP-ANOMALY-AND-BAFFLED
<BR>
<DD>IMXP-ANOMALY-EXPLAINED
<BR></DL>

</DL>

</DL>


<P>These representations are the most complicated knowledge structures in the program.  To appreciate the complexity of these representations consider the representation for IMXP-BAFFLED-AND-RESOLVED in <A HREF="repr-chapt.html#REF67816">Figure 30 on page 89</A>.  Although the frame definition takes an entire page in eight point font, the figure is still incomplete.  Some slots were removed in order to fit the page.  Moreover, the IMXP is one of the more moderately sized IMXPs, not the largest.  
<P>The following section explains what failure-causes the system actually considers when making an assignment of blame.  
<H4><A NAME="REF10980"> 8.5.2  The Implemented Space of Explanation Failures </A></H4>
<P>As currently implemented, the blame-assignment phase of learning does not consider all of the failure causes enumerated in <A HREF="symptoms-chapt.html#REF13157">Table 5, "Detailed taxonomy of causes of reasoning failure," on page 53</A>.  However, as mentioned in the introduction to this chapter, the implementation does consider many more of these causes than do most AI systems.  At the present time, the system concentrates on errors that arise from missing and flawed domain information and the indexing of such information in the BK, that is, the "Knowledge States" columns of <A HREF="symptoms-chapt.html#REF13157">Table 5</A>.  Yet given this limitation, the combinations of failure encountered are many (see <A HREF="meta-aqua-chapt.html#REF18716">Figure 79</A>), and, as will be explained in Section <A HREF="meta-aqua-chapt.html#REF28403">8.5.3, "Learning about higher-order knowledge,"</A> the resultant learning can be non-trivial.  
<P><A HREF="meta-aqua-chapt.html#REF18716">Figure 79</A> graphically illustrates the space of failure causes that blame assignment considers in the experimental study presented in the next chapter.  The shaded portion of the figure represents Meta-AQUA's performance system when no failures are detected.  The program first accepts a given input.  If the input is anomalous, the system explains it, otherwise it checks to see if the input is in any other way interesting.  If it is interesting, the system explains it; otherwise, it skims the input and accepts another.  Outside of the shaded area represents the space of failures.
<P><A HREF="meta-aqua-chapt.fig_34.ps" NAME="REF18716"><IMG SRC="meta-aqua-chapt.fig_34.gif"><P> Figure 79.  Implemented space of reasoning faults </A>




<P>When Meta-AQUA generates an explanation for an anomaly in the input story, the explanation may be incorrect.  Alternatively, it may reach an impasse when trying to explain and thus not be able to generate an explanation at all.  In the first case the explanation may be wrong, but the right explanation was in memory all along.  If it cannot explain an anomaly, the explanation may have existed, but could not be found.  All of these cases can occur both when the input is anomalous (left hand side of the figure) and when simply interesting (right hand side of the figure).  An additional case occurs when Meta-AQUA explains an anomalous input correctly.  It can then learn what was wrong with the knowledge such that it thought it was anomalous when actually it was not.  
<P>The circles at the bottom list the combinations of failure that occur given the situation (i.e., whether the explanation was given, etc.).  The shaded circle represents the fault of the hand coded story HC1 as described in Chapters VI and VII.  Annotations underneath the circles refer to stories described in previous or subsequent sections.  
<P>One may object that because these all map to a single fault, a decision tree could be built rather than going through the introspective process.  However, this figure represents the conditions available only in hindsight or through the auspices of an oracle; it is a virtual flow-chart, not an actual flow of control in the program.  Meta-AQUA must go through the blame-assignment process in order to determine the actual situation that applies to a given set of circumstances.  For example, there is no way that the system can determine in advance that it has the right explanation in memory but failed to find it (i.e., has forgotten the explanation and so the error is missing association).  A set of if-then statements will not suffice to perform blame assignment.  
<P>This set of failure combinations represents an exhaustive set of causes that can account for reasoning failure assuming correct processes, goals and input.  Adding these other dimensions makes the blame-assignment task much more complex.  However, we believe that deriving representations for these additional combinations will be tractable given a full analysis of the possibilities.  Heuristics must developed, however, to estimate when the assignment of blame will be so difficult that metareasoning should not be pursued (see next chapter).  
<P>A number of other researchers have presented learning approaches to many of the other failure types not covered directly in the Meta-AQUA implementation, although they do not specifically consider the learning-strategy construction problem.  Given this research, we can outline a number of relevant approaches to failure causes from <A HREF="symptoms-chapt.html#REF13157">Table 5</A> not covered in this implementation.  Research from these areas can be used in the future to make Meta-AQUA more complete.

<P>· <I>Input Noise</I>: The reasoner may possess the right knowledge, have it organized in a proper manner, and use the correct reasoning methods, yet fail due to incorrect or incomplete external knowledge sources.  In reasoning tasks, the blame may be due to measurement errors, obsolete data, missing data, or explicit deception by another agent.  The learning solution is to determine the conditions under which knowledge sources are reliable and the kinds of data that are necessary in a given situation (Booker, Goldberg &amp; Holland, 1989).  Data from human studies can be useful here to constrain the learning (e.g., see Johnson &amp; Seifert, in press).  
<P>· <I>Incorrect Reasoning Choice</I>: This failure type occurs when the reasoner has an appropriate knowledge structure with which to reason and an index to the structure in memory, but incorrectly chooses the wrong knowledge because the reasoning method it decided to use turned out to be inappropriate or inapplicable.  An analysis of the choice of reasoning methods results in learning control strategies designed to modify the heuristics (or add new heuristics) used in this choice (Mitchell et al., 1983; Sleeman, Langley &amp; Mitchell, 1984).
<P>· <I>Flawed behavior / Missing behavior</I>: The fault may occur because of incorrect procedural knowledge.  Stroulia (1994) has presented an interesting metaphor that pertains directly to these failure causes.  She treats a cognitive system as a device having reasoning components and models them with structure-behavior-function (SBF) models.  This allows her Autognostic system to perform blame-assignment with the SBF models in the same manner that other systems diagnose physical devices in the real world.  Such reflective diagnoses enables self-repair (learning).
<H4><A NAME="REF28403"> 8.5.3  Learning about higher-order knowledge</A></H4>
<P>As described by <A HREF="meta-aqua-chapt.html#REF50302">Section 8.3.3</A>, Tale-Spin outputs a CD representation of events in the story, and a translator converts these concepts into a frame representation that Meta-AQUA understands.  Often the input from Tale-Spin does not match Meta-AQUA's conceptual definitions and so the system detects an anomaly (e.g., the input dog-barks concept is anomalous because the object at which the dog barks is animate whereas the conceptual definition from which the system compares the input constrains the object slot to inanimate).  However, Meta-AQUA's conceptual knowledge for these primitive representations is not the only source of incorrect domain knowledge in the system.  As explained in <A HREF="meta-aqua-chapt.html#REF76396">Section 8.2</A>, during conceptual skimming a script application mechanism interprets the primitive acts given to it by using hierarchical knowledge from scripts.  Therefore, the inferences generated by these knowledge structures themselves may also contain errors that lead to failure.

<P>For instance, consider story TS3 in <A HREF="meta-aqua-chapt.html#REF88159">Figure 80</A>.  To represent sentence S26 ("Elvis smokes pot."), Tale-Spin generates the CD primitive INGEST whose actor is Elvis and whose object is marijuana.<A HREF="meta-aqua-chapt.html#FN17">(17)</A> This representation itself causes no anomaly because the actors of INGEST are volitional agents and the objects may be plants.  But, the script applier incorporates the INGEST into a Smoke-Pipe scene of the Smoking-Script.  It then examines the Smoke-Pipe scene for interesting input (and to see if it answers any previous questions).  Now because Smoke-Pipe has a constraint limiting the ingredients of pipes to be tobacco, an anomaly will result because of the knowledge in the scene's definition, not the definition of INGEST.  
<P><A HREF="meta-aqua-chapt.fig_79.ps" NAME="REF88159"><IMG SRC="meta-aqua-chapt.fig_79.gif"><P> Figure 80.  Tale-Spin story TS3</A>


<P>Following the detection of the anomaly, the system then asks why Elvis smokes the marijuana.  As a consequence, Meta-AQUA will use a tobacco smoking explanation to answer why Elvis chose to perform such an action.  The resultant hypothesis is that the action of smoking the pot relieves the tension of withdrawal from the effects of an addictive substance.  This explanation is later confirmed in the story (at S36), so the explanation is accepted.  The subsequent learning uses an IMXP called IMXP-ANOMALY-EXPLAINED to change by abstraction the constraint of the smoking scene to be any plant, rather than just tobacco (unfortunately an overgeneralization).  
<P>So although only one failure type was involved (incorporation failure) and only one learning algorithm needed (abstraction), even simple learning can be complex.  The problem is complex because inference is involved rather than just matches against concepts in the BK.  

<H4><A NAME="REF83295"> 8.5.4  Forgetting a Learned Explanation </A></H4>
<P>Even when a system learns new concepts or changes old ones, the successful use of this information may not always occur if the BK is not organized to promote the retrieval of it when needed.  The examples of <A HREF="theories-chapt.html#REF78268">Section 2.1</A> illustrated that, even though Meta-AQUA may learn a new explanation in one story, it can fail to reuse the explanation in a subsequent story.  A retrieval impasse can occur because, given the cues that compose a probe into memory, the existing explanation can lack the proper index with which to traverse the BK and thus to find the item.  In effect, Meta-AQUA can forget learned explanations and can expect instead to acquire this knowledge anew.  When Meta-AQUA is given the correct old explanation, the system must be reminded of the explanation that already exists in memory.  Therefore, rather than pursuing a goal to acquire and expand the knowledge, it must be able to switch to a goal of reorganizing the knowledge.  That is, learning goals are not static; even a system that uses the introspective method of deciding what to learn must be prepared to change its learning goals dynamically.
<P>Consider the earlier scenario examined in Chapters VI and VII.  The hand-coded story, HC1 (also the automatically generated story, TS1), represents a particular configuration of events.  In the story, a police dog barks at some luggage in an airport (or at a vase in Elvis' home in TS1).  Meta-AQUA considers the event to be anomalous because the system believes that dogs bark only at animate objects.  As was seen earlier, the program eventually learns that dogs can bark at any physical object, including inanimate ones.  It also learns the new explanation that "dogs bark when detecting contraband." After processing HC1 (TS1), Meta-AQUA's memory contains knowledge of two explanations for why dogs bark: an explanation for dogs that bark because they are threatened (indexed by <CODE>dog-barks-at-animate-object</CODE>) as well as an explanation for dogs that bark because they detect contraband (indexed by <CODE>dog-barks-at-container</CODE>).<A HREF="meta-aqua-chapt.html#FN18">(18)</A> 
<P>Tale-Spin then generates story TS4 (see <A HREF="meta-aqua-chapt.html#REF74753">Figure 81</A>) and outputs it to Meta-AQUA.  In this story, Elvis and Lynn are about to play with the ball when the police arrive at the house with a canine unit.  The dog immediately goes to a throw rug and sniffs at the object (S23).  When the dog barks (S24) the officer pulls back the rug to find Elvis' stash of marijuana.  Consequently, the officer arrests Elvis (S30) and takes him away (S32, S33).  The story then informs the reader that the dog barked because it detected the contraband (S35).  Because Elvis looses his freedom due to the arrest, he can no longer play ball with Lynn, and so he remains bored (the original problem that motivated the story). 
<P><A HREF="meta-aqua-chapt.fig_19.ps" NAME="REF74753"><IMG SRC="meta-aqua-chapt.fig_19.gif"><P> Figure 81.  Tale-Spin story TS4</A>



<P>Immediately after the dog barks at the carpet, Meta-AQUA generates a question to explain why the dog barked (see figure <A HREF="meta-aqua-chapt.html#REF61178">Figure 82</A>).  The reason for this decision is that the system has recently learned about dogs and barking, so it is interested in any subsequent information that may be related.  However, because the dog is barking at a rug and such an object is not a container, it does not retrieve the newly learned detection explanation.  The dog also is not barking at an animate object, so the old threaten explanation is not retrieved.  Instead, it can generate no explanation to explain the interesting story concept.
<P><A HREF="meta-aqua-chapt.fig_32.ps" NAME="REF61178"><IMG SRC="meta-aqua-chapt.fig_32.gif"><P> Figure 82.  Meta-AQUA output during hypothesis generation phase of TS4</A>


<P>Reviewing the reasoning trace that preceded the conclusion, Meta-AQUA characterizes itself as "baffled" (impasse during memory retrieval).  The system retrieves an IMXP based on this characterization, which helps it explain its own reasoning failure.  The structure is unified with the representation of the original reasoning (stored in a TMXP) which produces the instantiation partially shown in <A HREF="meta-aqua-chapt.html#REF95129">Figure 83, "Instantiated forgotten detection explanation."</A> The knowledge structure shows that memory retrieval produced no explanation in response to the system's question.  Instead, a later input produced the answer.  
<P><A HREF="meta-aqua-chapt.fig_30.ps" NAME="REF95129"><IMG SRC="meta-aqua-chapt.fig_30.gif"><P> Figure 83.  Instantiated forgotten detection explanation</A>




<P>The IMXP suggests that a knowledge-expansion goal be spawned to generalize the input explanation.  This suggestion comes from the potential-learning-goal slot of the IMXP (see <A HREF="repr-chapt.html#REF67816">Figure 30., "IMXP frame definition for forgetting"</A> on <A HREF="repr-chapt.html#REF67816">page 89</A>).  Conditions attached to the knowledge-expansion goal allow it to be posted if the node A was either acquired from the story or inferred, but not if it was retrieved from memory.  A knowledge-organization goal is also spawned in order to index the generalized explanation in memory.  These goals can be achieved by performing explanation-based generalization (EBG) on the new explanation (node A) and then indexing the explanation by the context in which the system encountered the explanation.
<P>The system cannot determine <I>a priori</I> whether an abstract XP (node M) actually exists in memory but could not be recalled (thus, the failure cause is a missing association, I), or whether the system lacks the knowledge to produce the explanation (thus, the cause is that the situation is novel, i.e., M is missing).  The system thus poses a question about its own IMXP (cf. Oehlmann, Edwards &amp; Sleeman, 1994), "Does M exist in memory?" If M is missing, I is also missing; thus, the right question to ask is whether M exists, not I.<A HREF="meta-aqua-chapt.html#FN19">(19)</A>

<P>The answer to the introspective question is obtained by performing EBG and then watching for a similar explanation in memory when it stores the new explanation via the indexing algorithm.  As briefly described in <A HREF="meta-aqua-chapt.html#REF83198">Section 8.4.2</A> (specifically see <A HREF="meta-aqua-chapt.html#REF0">page 194</A>), the system can detect the presence of similar memories by maintaining a list of pointers to memory items for each conceptual type.  At storage time, Meta-AQUA traverses the list, checking each to see if it can unify the new memory with any of the older ones.<A HREF="meta-aqua-chapt.html#FN20">(20)</A> Meta-AQUA thus finds the explanation produced by the previous story (see <A HREF="meta-aqua-chapt.html#REF30003">Figure 84</A>).  
<P><A HREF="meta-aqua-chapt.fig_59.ps" NAME="REF30003"><IMG SRC="meta-aqua-chapt.fig_59.gif"><P> Figure 84.  Learning phase during TS4</A>


<P>Merging the two explanations produces a better explanation: Dogs may bark at objects that hide contraband, not just at containers that hold contraband.  The algorithm that indexes the generalization searches for the common ancestor of the object slots of both explanations; that is, objects that contain other objects and objects that cover other objects.  This common ancestor is the type <CODE>hiding-place</CODE>.  Thus, so that these types of explanations will not be forgotten again, the system indexes the explanation by "dogs that bark at potential hiding places" and places a pointer to the merged explanation on the memory list for the symbol <CODE>causal-relation</CODE>.  
<P>As a result of its learning, Meta-AQUA not only detects no anomalies in stories such as HC3 (<A HREF="meta-aqua-chapt.html#REF70745">Figure 85</A>), it predicts the correct explanation in S7 while processing S5.  Most importantly, however, this story illustrates the fact that learning goals are not static, but rather, that they are subject to dynamic re-evaluation, even when the planner that creates a learning plan knows about interactions.  Some facets that bear on the pursuit of learning goals cannot always be anticipated in advance.  In this case, the system had decided that it needs to acquire a new piece of knowledge, but instead it discovers that it had the knowledge all along.  So instead of achieving a knowledge expansion goal to generalize and store what it thinks is a new explanation, it rediscovers the old one and changes the learning goal to a knowledge organization goal. 
<P><A HREF="meta-aqua-chapt.fig_56.ps" NAME="REF70745"><IMG SRC="meta-aqua-chapt.fig_56.gif"><P> Figure 85.  Subsequent test story (HC3)</A>




<H3><A NAME="REF57764"> 8.6   Summary</A></H3>
<P>This chapter described the Meta-AQUA implementation including the four major subsystems (not including the frame representation system).  These are the performance subsystem, the input subsystem, memory, and the learning subsystem.  The performance system is story understanding.  Among new implementational details this chapter discussed concerning the performance system was the script application module and its implication for learning about higher-order knowledge.  The input system not only provides hand-tailored stories as seen in previous chapters, but, as introduced in this chapter, it also includes an automatic story generator called Tale-Spin.  The memory system is an indexed memory divided into the BK and FK.  An important feature of the memory is that opportunistic remindings are supported so that multiple hypothesis formations and verifications can be interleaved within a series of inputs.  We also saw that Meta-AQUA can benefit from learning in a particular story before the story is even finished.  
<P>This chapter also reintroduced the learning subsystem.  When discussing the learning system, an example of forgetting illustrated a number of points.  First, forgetting can occur when the indexes for items are poorly organized in memory.  Secondly, and more importantly, even if it knows about interactions between learning methods, the learning system must be prepared to change dynamically the learning goals being pursued.  Finally, the example demonstrated the utility of introspective questions.
<P>Markovitch and Scott (1993) characterize learning systems in terms of filters placed in an information flow through a system.  Meta-AQUA possesses an input bias at the front end in the information flow; that is, the bias is to prefer failed experience.  Markovitch and Scott call such a filter <I>selective experience</I>.  They divide selective experience into three types: error-based, uncertainty-based, and miscellaneous heuristics.  The examples presented in this thesis are all error-based, although the scope of the selective-experience filter in Meta-AQUA goes beyond their formulation because, as explained in <A HREF="symptoms-chapt.html#REF40941">Chapter III</A>, error has numerous variations, only one of which (contradiction) Markovitch and Scott consider.  Moreover, they claim that error-based filters are useful only when the input is in the form of problem/solution tuples.  During the impasse of story TS4 (<A HREF="meta-aqua-chapt.html#REF83295">Section 8.5.4</A>), however, Meta-AQUA generates no solution, yet the system was still able to learn a valuable lesson from the experience.

<P>Meta-AQUA filters input examples in a relatively passive manner.  It waits for failures to occur, then processes them by explaining the failure, deciding what to learn, and constructing a learning strategy.  Another issue to pursue would be to have the system try to actively generate failed experiences in order to test or disprove some hypothesis or to generate learning experiences for some performance task.  As <A HREF="appendix-a.html#REF24995">Appendix A</A> asserts, however, the system must be sensitive to inductive policies concerning the task domain.  Currently, the ability of a system to actively challenge itself and its knowledge is beyond the scope of our research.  
<P>With the implementational details presented by this chapter in hand, the next chapter can now examine how the total system is evaluated in an empirical study of the benefits of introspective multistrategy learning.  

<HR><H3>Footnotes</H3>
<DL COMPACT>
<DT><A NAME=FN1>(1)</A><DD>See Section 8.5 for a specification of the scope of these assumptions with respect to the failure causes of Table5, "Detailed taxonomy of causes of reasoning failure," on page53.  
<DT><A NAME=FN2>(2)</A><DD>This definition does not include some minor details such as the statements that determine module dependencies (i.e., :in-order-to clauses).  
<DT><A NAME=FN3>(3)</A><DD>Meta-AQUA actually has three performance modes from which it can operate.  This chapter discusses Meta-AQUA performance in read-story mode (story understanding).  But in addition, the system can be set to a LISP-programming mode in which the system attempts to create and understand LISP code (see Section 9.3).  A third mode, act-out-story, simulates goal-driven planning behavior between two interacting characters.  The characters represent a custom official and a smuggler who attempt to act out the dynamics of story HC1 in the airport terminal.  Although this partially implemented mode is mostly outside of the scope of this document, see Chapter XIII for speculations regarding the interaction of planning and understanding and the potential contribution of this performance mode within the system.  
<DT><A NAME=FN4>(4)</A><DD>Section 8.4 briefly describes how such processes are resumed.  
<DT><A NAME=FN5>(5)</A><DD>At URL <A HREF="ftp://ics.uci.edu/pub/machine-learning-programs/TalespinOccam">ftp://ics.uci.edu/pub/machine-learning-programs/TalespinOccam</A>, the code is publicly available through the World Wide Web. 
<DT><A NAME=FN6>(6)</A><DD>In the vernacular, a "jones" is a drug habit accompanied by withdrawal symptoms.  The verb "to jones" is to be going through a state of withdrawal.
<DT><A NAME=FN7>(7)</A><DD>Unlike the UC Irvine version of Tale-Spin in which characters and their goals did not interact, the program has been modified so that the police officer is a competing character with his own problem and goal as he arrives on the scene.  Because the police will confiscate the marijuana when found and then arrest Elvis, such events may preempt the enabling conditions of actions Elvis had planned to perform.  For instance, if Elvis is thirsty but the officer arrests him, this condition restricts his freedom of movement so that he cannot go to the faucet for water.  Therefore, the story can end with Elvis still having the problem with which he began (i.e, thirst).  
<DT><A NAME=FN8>(8)</A><DD>Such an XP explains that a particular actor chose to perform a particular action because of the goal to achieve a state for which the action results.  See Figure22 on page76 for a representation of such XPs.  
<DT><A NAME=FN9>(9)</A><DD>The reason for this property is that literals have no sub-frame value.  
<DT><A NAME=FN10>(10)</A><DD>For details about similar approaches to knowledge representation, see Jones (1992), Ram (1989), and Wilensky (1986b).
<DT><A NAME=FN11>(11)</A><DD>In the frame knowledge representation system, function abstractions exist to compute this mapping.  The call (*FRAME* frame-var) retrieves a token's value, whereas (frame-type frame-var) returns the type of a given token.  
<DT><A NAME=FN12>(12)</A><DD>Actually, the implementation uses a call to the LISP function gentemp so that the symbol will be interned into the current symbol package.  
<DT><A NAME=FN13>(13)</A><DD>Note the similarity between story TS2 and story HC1 ("The handball game") in Figure69 on page170.  
<DT><A NAME=FN14>(14)</A><DD>The second index would be represented as actor-->hit-->child-->to-->ball-->question-type.0-->q, where q is the question be indexed.  
<DT><A NAME=FN15>(15)</A><DD>The input XP is of type XP-GOAL-OF-OUTCOME-&gt;ACTOR which states that "if the expected outcome of an action results in a goal state for that agent, then the agent will choose to perform that act." This is a basic assumption of rationality (Newell, 1982).  See Ram (1989, 1994) for additional details concerning volitional XPs that explain why agents perform particular classes of actions.  
<DT><A NAME=FN16>(16)</A><DD>Obtained from the University of Maryland at College Park at URL ftp://cs.umd.edu/pub/nonlin in file nonlin-files.tar.Z.  
<DT><A NAME=FN17>(17)</A><DD>For a full trace of the system behavior under this
example, see <A HREF=appendix-b.html#REF75687">Appendix B</A>.  
<DT><A NAME=FN18>(18)</A><DD>See Section 8.4 for the representation of these two indexes, especially Figure 77.  
<DT><A NAME=FN19>(19)</A><DD>Note that it cannot be the case that I is erroneous.  If it were true, then some explanation would have been retrieved, although it may have been inappropriate.  
<DT><A NAME=FN20>(20)</A><DD>This mechanism simulates a memory such as that of DMAP (Martin, 1989, 1990), whereby memory items map to areas that contain similar memories.  Although Meta-AQUA's mechanism is only a crude approximation to such architectures, the emphasis of IML theory is on the reasoning about memory (or other reasoning processes), rather than on a representation of the memory architecture per se.  A more realistic mechanism would be for Meta-AQUA to use the generalized XP as a probe to memory to see if it is now reminded of the old XP.  The current method suffers from the fact that it always finds the old XP at an unacceptable search cost.  
</DL>
<P><A HREF="git-cc-96-06.book.html"><IMG SRC="fm2html-toc.gif">Table of Contents</A>
<A HREF="eval-chapt.html"><IMG SRC="fm2html-next.gif">Next Chapter</A>

<HR>

<A HREF="http://www.cc.gatech.edu/aimosaic/students/Ai-students/cox/cox.html"><IMG
ALIGN=MIDDLE
SRC="http://www.cc.gatech.edu/aimosaic/students/Ai-students/cox/Www/Images/home2.gif"></A>

<A HREF="http://www.cc.gatech.edu/cogsci">
<IMG ALIGN=MIDDLE
SRC="http://www.cc.gatech.edu/aimosaic/students/Ai-students/cox/Www/Images/cogsci-granite3.gif"></A>